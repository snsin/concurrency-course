# Задание 2. Немного вопросов на понимание #

1. Почему паттерн Reduction больше подходит для многопоточных алгоритмов, чем Accumulator?

2. Для каких Stream API операций НЕЛЬЗЯ применить подход с редукцией?
    - sorted
    - distinct
    - flatMap
    - filter
    - count
    - collect
    - Для всех можно

3. Почему для ConcurrentHashMap используется расчет количества подзадач, а не деление на основе количества элементов?

#### Ответы ####

1. Потому, что в случае с паттерном Reduction результат выполнения накапливается в каждом потоке независимо,
   и также последующее объединение происходит для каждой группе (паре) задач независимо. Таким образом снижается
   количество обращений к общим переменным, а вслед за этим и количество блокировок, и накладные расходы на
   синхронизацию.

2. Сложный вопрос, на мой взгляд, попробую проанализировать каждую из операций:
    - sorted - считаю, что здесь всё ок, сортировку вполне можно проводить независимо для подзадач,
      а потом объединять результаты (merge sort)
    - distinct - в принципе также можно разделить задачу на несколько подзадач, обработать их так, чтобы
      в результате выполнения каждой подзадачи образовался стрим уникальных элементов, и потом, при объединении,
      также применять оператор distinct
    - flatMap - здесь сомневаюсь, но мысли следующие - кажется, вполне возможно разделить стрим из стримов на
      несколько подзадач, применить функцию к каждой подзадаче и далее объединить стримы результирующего типа.
    - filter - тоже кажется, что можно отфильтровать элементы внутри подзадачи, а затем объединить результаты
      например, если есть стрим из целочисленных элементов от 1 до 1 000 000 и есть задача оставить только четные,
      можно разбить его на стримы, допустим по 10 000 элементов, внутри каждого выполнить фильтрацию и затем объединить
      результат, например конкатенировать стримы.
    - count - всё также - разделить на несколько задач, для каждой позадачи сделать подсчет, а потом сложить
      результаты подзадач
    - collect - аналогично предыдущему.
    - Для всех можно, в итоге мой вывод, что для всех операций можно применить подход с редукцией, другой вопрос
      насколько в каждом из случаев это будет эффективно.

3. Деление на основе количества элементов в ConcurrentHashMap потребует сложных вычислений, что не всегда уместно.
   Также, при делении на основе количества элементов может получиться так (и скорее всего получится), что элементы
   для одной задачи находятся в разных бакетах, с разной структурой (например бакет из одного элемента и бакет
   с деревом элементов), причем, может получиться так, что в одном бакете содержатся элементы для нескольких задач.
   В этом случае доступ к элементам будет сложно обеспечить, потребуется дополнительная синхронизация,
   т.к. в ConcurrentHashMap синхронизация выполняется по первому элементу бакета. В результате никакого прироста
   производительности от параллельного выполнения задач может и не быть, наоборот может получиться так,
   что производительность снизится. 